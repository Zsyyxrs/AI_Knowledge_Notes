
# 一、特点

**适用场景:**
- 需要改变模型的**输出风格、语气或格式**时(比如客服对话风格、特定写作风格)
- 需要模型学习**特定领域的推理模式**或专业知识(如医疗诊断逻辑、法律分析)
- 有充足的**高质量标注数据**(通常需要几百到几千条)
- 知识相对**稳定**,不需要频繁更新
- 需要模型"内化"某种能力,而不只是知道事实

**优势:**
- 可以深度改变模型行为和输出风格
- 推理时延低,不需要额外检索步骤
- 适合学习复杂的推理模式
- 部署后无需外部知识库

**劣势:**
- 需要大量高质量训练数据和GPU资源
- 更新知识需要重新训练,成本高
- 可能产生幻觉(hallucination),编造不存在的信息
- 难以追溯信息来源
- 训练周期长,迭代慢

**优先选择微调:**
- 需要特定输出格式(如结构化数据生成)
- 客服机器人(需要特定语气和话术)
- 特定领域的专业任务(如代码生成、翻译)
- 有充足训练数据和预算
- 对延迟敏感的应用

**混合使用(最佳实践):** 实际项目中,微调和rag结合往往效果最好:
- 用RAG提供准确的事实性知识
- 用微调优化输出格式和交互风格
- 例如:微调模型学习如何组织和呈现信息,RAG提供最新准确的内容

 **决策流程**
1. 先评估是否有足够的高质量训练数据?没有→优先RAG
2. 知识是否需要频繁更新?是→优先RAG
3. 主要需求是改变输出风格还是补充知识?前者→微调,后者→RAG
4. 预算和技术资源如何?有限→RAG
5. 是否需要信息溯源?是→RAG
# 二、问题累积


1. 如何处理长文本训练问题

| **方案类别**                    | **方法**                                                                  | **核心思想**                | **适用场景**      |
| --------------------------- | ----------------------------------------------------------------------- | ----------------------- | ------------- |
| **A. 截断 / 滑动窗口**            | 分段输入                                                                    | 取前512字，或滑动窗口分块          | 短文本任务，简单快速    |
| **B. 层次化建模 (Hierarchical)** | Sentence-level + Document-level                                         | 先编码句子，再聚合句子表示           | 文档分类、情感分析     |
| **C. 长上下文模型**               | Longformer / BigBird / RoPE扩展                                           | 改进注意力机制支持 4k~32k tokens | 有长上下文需求的任务    |
| **D. 向量分块 + 聚合**            | RAG / Embedding Pooling                                                 | 每块取向量 → 平均或最大池化         | 检索类、文本匹配      |
| **E. RLHF / Adapter 拓展**    | 多轮提示微调                                                                  | 通过prompt控制输入分段          | LoRA微调多轮长文本任务 |
| **F. 数据预处理**                | 分句、去噪、压缩                                                                | 去掉无用信息、合并核心句            | 任何长文本任务       |
| **G.修改配置文件+全量微调**           | 修改 config.json 扩展 max_position_embeddings 或 rope_scaling →然后进行 **全量微调** | 使得原来的最大token长度变大        | GPU资源充足时      |
