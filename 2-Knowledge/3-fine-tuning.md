# 一、问题累积


1. 如何处理长文本训练问题

| **方案类别**                    | **方法**                                                                  | **核心思想**                | **适用场景**      |
| --------------------------- | ----------------------------------------------------------------------- | ----------------------- | ------------- |
| **A. 截断 / 滑动窗口**            | 分段输入                                                                    | 取前512字，或滑动窗口分块          | 短文本任务，简单快速    |
| **B. 层次化建模 (Hierarchical)** | Sentence-level + Document-level                                         | 先编码句子，再聚合句子表示           | 文档分类、情感分析     |
| **C. 长上下文模型**               | Longformer / BigBird / RoPE扩展                                           | 改进注意力机制支持 4k~32k tokens | 有长上下文需求的任务    |
| **D. 向量分块 + 聚合**            | RAG / Embedding Pooling                                                 | 每块取向量 → 平均或最大池化         | 检索类、文本匹配      |
| **E. RLHF / Adapter 拓展**    | 多轮提示微调                                                                  | 通过prompt控制输入分段          | LoRA微调多轮长文本任务 |
| **F. 数据预处理**                | 分句、去噪、压缩                                                                | 去掉无用信息、合并核心句            | 任何长文本任务       |
| **G.修改配置文件+全量微调**           | 修改 config.json 扩展 max_position_embeddings 或 rope_scaling →然后进行 **全量微调** | 使得原来的最大token长度变大        | GPU资源充足时      |