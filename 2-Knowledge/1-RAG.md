# 一、简介
RAG（Retrieval Augmented Generation）顾名思义，通过**检索**的方法来增强**生成模型**的能力。
搭建过程：
1. 文档加载，并按一定条件切分成文本块
2. 通过embedding模型转换成向量
3. 在向量数据库里根据向量相似度检索得到最高的几组答案
4. 通过query和answer构建prompt给大模型最终得到回复

![](https://cdn.jsdelivr.net/gh/Zsyyxrs/picgo-images/img/rag.png)

离线步骤：
1. 文档加载
2. 文档拆分
3. 向量化
4. 灌入向量数据库
## 在线步骤：
1. 获得用户问题
2. 

# 二、文本切分
pdfminer.six或者pdfplumber解析pdf提取文字等
RAGflow：
普通文本切割：\n 等基于一定规则切分
复杂文本：NSP任务进行微调训练

文本切分的**chunk_size**对于 大多数文档问答型 RAG：推荐区间：chunk_size = 500 ~ 1000 tokens，如果用字符计（英文）约为 2000 ~ 4000 chars
**overlap_size**通常设为 chunk_size 的 **10% ~ 20%**， 如果文档段落很长或句子之间语义依赖强（如故事、法规条款），可以适当增大到 **25% ~ 30%**。 如果是 FAQ、独立条目，可以降低到 **0~50 tokens**。

不同环境下可以放不同的 .env 文件：
.env.dev
.env.prod
load_dotenv(".env.dev")
find_dotenv()在当前路径和父路径找，返回完整路径
不要上传 .env 到 GitHub ，在 .gitignore 中加一行 .env

# 三、文本嵌入和检索
构建向量数据库：选择合适的向量数据库如chromadb、milvus等

| 名称           | 特点                          | 是否开源 | 推荐场景          | 索引支持更新？  | 更新速度  | 索引更新机制                      | 备注          |
| :----------- | :-------------------------- | ---- | :------------ | -------- | ----- | :-------------------------- | :---------- |
| **FAISS**    | Facebook 开源，支持高效 ANN，C++ 实现 | ✅    | 本地部署，研发场景     | ❌（原生不支持） | ❌ 慢   | 需重建索引                       | 适合静态数据      |
| **Milvus**   | 专为大规模向量数据设计，支持多种索引          | ✅    | 大规模医学图像或文本库   | ✅        | ✅ 较快  | 异步增量索引，删除为懒惰删除（lazy delete） | 插入后自动增量索引   |
| **Weaviate** | 自带向量模型，RESTful API 易集成      | ✅    | 快速上线，文档结构化    | ✅        | ✅ 快   | 实时索引，支持 payload 更新          | 非阻塞更新，良好性能  |
| **Qdrant**   | 支持 payload，Python 接口友好      | ✅    | 多模态向量 + 元信息检索 | ✅        | 中等    | HNSW 支持增删改，但需维护结构           | 自动同步更新索引    |
| **Pinecone** | 云服务为主，自动分布式扩展               | ❌    | SaaS 快速构建原型   | ✅        | 商业级优化 | 自有索引机制                      | SaaS，隐藏内部细节 |
| **Chroma**   | 轻量本地，LangChain 默认集成         | ✅    | 小项目 / 测试      | ✅（有限支持）  | 较快    | 简单内存索引或 SQLite 持久化          | 适合个人项目或原型验证 |
选择合适的embedding（文本嵌入）模型：

| 模型                           | 开发/提供方                   | 向量维度      | 语言支持      | 特点                           | 局限               | 推荐场景                |
| ---------------------------- | ------------------------ | --------- | --------- | ---------------------------- | ---------------- | ------------------- |
| **text-embedding-3-large**   | OpenAI                   | 3072      | 多语言（英文最佳） | 精度最高；语义表达强；OpenAI 官方支持；稳定API | 成本高；黑箱；需联网       | 企业级 RAG、语义检索、高质量知识库 |
| **text-embedding-3-small**   | OpenAI                   | 1536      | 多语言       | 性能/成本平衡佳；API 使用方便            | 精度略低于 large；仍需付费 | 中小规模知识库、快速检索场景      |
| **intfloat/e5-large-v2**     | 开源（Google / 社区）          | 1024      | 多语言（尤其中英） | 开源可本地部署；性能稳定；通用性强            | 需 GPU 资源；速度略慢    | 自主部署、RAG 检索、跨语言搜索   |
| **GTE-Large (Qwen2 版)**      | 阿里巴巴 NLP 团队              | 1024      | 多语言       | 中文优化好；可自定义微调                 | 开源生态较新           | 中文语义检索、垂直领域问答       |
| **jina-embeddings-v3**       | Jina AI                  | 1024（可调整） | 多语言       | 支持长上下文（8K+ tokens）；可自部署      | 新模型，社区尚小         | 长文档检索、多语知识库         |
| **Cohere Embed v3**          | Cohere（商用）               | ≈1024     | 多语言       | 高效长文本 embedding；商用支持好        | 需付费；不可自部署        | 企业检索、知识问答           |
| **all-MiniLM-L6-v2 (SBERT)** | SentenceTransformers（开源） | 384       | 多语言（英文最佳） | 快速轻量；Chroma 默认使用；免费          | 语义精度低于大型模型       | 原型验证、本地实验、小型项目      |

> 所有嵌入模型都在高维空间中用类似的余弦相似度度量语义接近性；真正的区别在于模型如何构建这个空间（embedding 维度、训练目标、语言范围等）。

**选型建议**

| 使用需求                      | 推荐模型                                 | 理由                              |
| ------------------------- | ------------------------------------ | ------------------------------- |
| ✅ **最高精度、生产级 RAG 系统**     | text-embedding-3-large               | 语义一致性最好，适合知识密集型场景（如金融、医疗、企业知识库） |
| ⚡️ **低成本高效检索**            | text-embedding-3-small               | 成本仅 1/8，大多数任务中性能接近 large        |
| 🧠 **可离线 / 私有化部署**        | e5-large-v2 或 all-MiniLM-L6-v2       | 无需联网；支持本地运行；适合隐私要求高的场景          |
| 🀄️ **中文优化任务**            | GTE-Large (Qwen2)                    | 语义理解更贴近中文，中文文档召回效果更好            |
| 📚 **超长文档检索（>8k tokens）** | jina-embeddings-v3 或 Cohere Embed v3 | 支持长上下文嵌入                        |
| 🧪 **快速实验 / 原型验证**        | all-MiniLM-L6-v2                     | 模型轻、推理快、资源占用小，Chroma 可直接调用      |
向量数据库里的索引

| 索引方式             | 形式                                                       | 特点                                               | 适合          |
| ---------------- | -------------------------------------------------------- | ------------------------------------------------ | ----------- |
| **Flat（暴力搜索）**   | 不做任何处理                                                   | 精确但慢                                             | 小规模         |
| **annoy**        | 多颗随机kd树，通过树数调整精度                                         | 降低精确，速度快                                         | 规模小         |
| **IVF（倒排文件）**    | 先粗后细（需先聚类），通过设置nprode个数调整精度                              | 常用于 FAISS，速度很快，精度高，内存中                           | 中等数据量，不支持插入 |
| **LSH（局部敏感哈希）**  | 计算距离变成查哈希表                                               | 查询极快，和IVF、PQ一样是候选生成+精排，精度中，内存高                   | 支持插入        |
| **HNSW（图索引）**    | 构造一个多层图（高层稀疏、底层密集），每个向量是图中的节点，搜索时像在地图上“导航”一样快速逼近目标       | 支持高性能 ANN，Qdrant/Milvus，速度超快，精度高，内存高，构建时间慢，不需要训练 | 实时检索，支持动态增删 |
| **PQ（乘积量化）/OPQ** | 有损压缩，把高维向量拆成多个子向量，对每个子向量用 KMeans 建立查找表，每个向量最终只用多个「子索引」表示 | 压缩向量减少内存占用，快，精度中，内存低                             | 大规模，不支持插入   |
| **DiskANN**      |                                                          | 存储在磁盘中                                           | 海量数据（数亿级）   |
元数据索引
不参与向量相似度计算，过滤数据，用来实现更复杂的查询逻辑
对于元数据，底层可能用：
- 倒排索引（Inverted Index）用于分类字段（如性别、标签）
- B+ 树 或 Segment Tree 用于范围字段（如年龄、时间）
- 位图索引 用于布尔字段（如是否活跃）

[可变长度embedding](https://arxiv.org/abs/2205.13147)

嵌入模型怎么拆分、训练的 
找项目相关的语料库用LLM进行评估

大多数开源的需要微调

向量嵌入：向量、原文、id的形式写入，列表形式全文档写入
向量检索：向量、top_n，也是列表形式

文本向量
1. 将文本转成一组*N*维浮点数，即**文本向量**又叫 Embeddings
2. 向量之间可以计算距离，距离远近对应**语义相似度**大小

文本向量训练
1. 构建相关（正例）与不相关（负例）的句子对样本
2. 训练双塔式模型，让正例间的距离小，负例间的距离大

![](https://cdn.jsdelivr.net/gh/Zsyyxrs/picgo-images/img/sbert.png)
检索后重排序




测试图片
![image.png](https://cdn.jsdelivr.net/gh/Zsyyxrs/picgo-images/img/20251027225616935.png)

向量相似度衡量指标
![](https://cdn.jsdelivr.net/gh/Zsyyxrs/picgo-images/img/sim.png)


向量数据库chroma :不指定embedding模型时用默认的sbert的all-MiniLM-L6-v2，384维
[比较语句相似度的SBERT](https://www.sbert.net/)

检索后重排序（rerank）
