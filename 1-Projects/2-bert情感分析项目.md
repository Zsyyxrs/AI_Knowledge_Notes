[bert模型地址](https://github.com/google-research/bert/blob/master/multilingual.md)
2 种语言，12 层，768 隐藏单元，12 个注意力头，110M 参数

训练过程：
- **type_vocab_size:** 2
- **vocab_size:** 21128
- **num_hidden_layers:** 12

huggingface
